{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ab0bd4",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mJupyter Server crashed. Unable to connect. \n",
      "\u001b[1;31mError code from Jupyter: 1\n",
      "\u001b[1;31m[I 2025-09-01 08:45:31.549 ServerApp] jupyter_lsp | extension was successfully linked.\n",
      "\u001b[1;31m[I 2025-09-01 08:45:31.552 ServerApp] jupyter_server_terminals | extension was successfully linked.\n",
      "\u001b[1;31m[W 2025-09-01 08:45:31.553 LabApp] 'iopub_data_rate_limit' has moved from NotebookApp to ServerApp. This config will be passed to ServerApp. Be sure to update your config before our next release.\n",
      "\u001b[1;31m[W 2025-09-01 08:45:31.555 ServerApp] ServerApp.iopub_data_rate_limit config is deprecated in 2.0. Use ZMQChannelsWebsocketConnection.iopub_data_rate_limit.\n",
      "\u001b[1;31m[I 2025-09-01 08:45:31.555 ServerApp] jupyterlab | extension was successfully linked.\n",
      "\u001b[1;31m[I 2025-09-01 08:45:31.558 ServerApp] notebook | extension was successfully linked.\n",
      "\u001b[1;31m[I 2025-09-01 08:45:32.445 ServerApp] notebook_shim | extension was successfully linked.\n",
      "\u001b[1;31m[I 2025-09-01 08:45:32.510 ServerApp] notebook_shim | extension was successfully loaded.\n",
      "\u001b[1;31m[I 2025-09-01 08:45:32.512 ServerApp] jupyter_lsp | extension was successfully loaded.\n",
      "\u001b[1;31m[I 2025-09-01 08:45:32.513 ServerApp] jupyter_server_terminals | extension was successfully loaded.\n",
      "\u001b[1;31m[I 2025-09-01 08:45:32.530 LabApp] JupyterLab extension loaded from /mnt/shared/yuntech_htchang_d11010212/new_brainseg/brain_env/lib/python3.10/site-packages/jupyterlab\n",
      "\u001b[1;31m[I 2025-09-01 08:45:32.530 LabApp] JupyterLab application directory is /mnt/shared/yuntech_htchang_d11010212/new_brainseg/brain_env/share/jupyter/lab\n",
      "\u001b[1;31m[I 2025-09-01 08:45:32.531 LabApp] Extension Manager is 'pypi'.\n",
      "\u001b[1;31m[I 2025-09-01 08:45:32.657 ServerApp] jupyterlab | extension was successfully loaded.\n",
      "\u001b[1;31m[I 2025-09-01 08:45:32.672 ServerApp] notebook | extension was successfully loaded.\n",
      "\u001b[1;31m[I 2025-09-01 08:45:32.672 ServerApp] The port 8888 is already in use, trying another port.\n",
      "\u001b[1;31m[C 2025-09-01 08:45:32.672 ServerApp] Running as root is not recommended. Use --allow-root to bypass. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import os\n",
    "\n",
    "# Dataset untuk memuat slice 2D tengah dari dataset BraTS2020. \n",
    "# Setiap item mengembalikan 4 channel MRI (flair, t1, t1ce, t2) dan mask segmentasinya.\n",
    "class BraTSDataset2D(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        self.base_path = os.path.join(data_dir, 'MICCAI_BraTS2020_TrainingData')\n",
    "        if not os.path.isdir(self.base_path):\n",
    "            raise FileNotFoundError(f\"Direktori tidak ditemukan di: '{self.base_path}'\")\n",
    "        self.patient_dirs = [p for p in os.listdir(self.base_path) if os.path.isdir(os.path.join(self.base_path, p))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.patient_dirs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        patient_dir_name = self.patient_dirs[idx]\n",
    "        patient_path = os.path.join(self.base_path, patient_dir_name)\n",
    "\n",
    "        # Path untuk setiap modalitas citra\n",
    "        flair_path = os.path.join(patient_path, f\"{patient_dir_name}_flair.nii\")\n",
    "        t1_path = os.path.join(patient_path, f\"{patient_dir_name}_t1.nii\")\n",
    "        t1ce_path = os.path.join(patient_path, f\"{patient_dir_name}_t1ce.nii\")\n",
    "        t2_path = os.path.join(patient_path, f\"{patient_dir_name}_t2.nii\")\n",
    "        seg_path = os.path.join(patient_path, f\"{patient_dir_name}_seg.nii\")\n",
    "\n",
    "        # Memuat data citra\n",
    "        flair_img = nib.load(flair_path).get_fdata()\n",
    "        t1_img = nib.load(t1_path).get_fdata()\n",
    "        t1ce_img = nib.load(t1ce_path).get_fdata()\n",
    "        t2_img = nib.load(t2_path).get_fdata()\n",
    "        seg_mask = nib.load(seg_path).get_fdata()\n",
    "\n",
    "        # Mengambil slice tengah dari volume 3D\n",
    "        mid_slice_idx = flair_img.shape[2] // 2\n",
    "        image_stack = np.stack([\n",
    "            flair_img[:, :, mid_slice_idx],\n",
    "            t1_img[:, :, mid_slice_idx],\n",
    "            t1ce_img[:, :, mid_slice_idx],\n",
    "            t2_img[:, :, mid_slice_idx]\n",
    "        ], axis=0)\n",
    "        seg_slice = seg_mask[:, :, mid_slice_idx]\n",
    "\n",
    "        # Konversi ke tensor PyTorch\n",
    "        image_tensor = torch.tensor(image_stack, dtype=torch.float32)\n",
    "        label_tensor = torch.tensor(seg_slice, dtype=torch.long)\n",
    "        \n",
    "        # Ganti label 4 menjadi 3 menggunakan masked_fill_ dengan mask boolean\n",
    "        label_tensor.masked_fill_(label_tensor == 4, 3)\n",
    "        \n",
    "        # Normalisasi sederhana per channel\n",
    "        for i in range(image_tensor.shape[0]):\n",
    "            max_val = torch.max(image_tensor[i])\n",
    "            if max_val > 0:\n",
    "                image_tensor[i] = image_tensor[i] / max_val\n",
    "\n",
    "        return image_tensor, label_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88a1f4ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device yang akan digunakan: gpu\n",
      "Mempersiapkan dataset...\n",
      "Total sampel (per pasien): 369\n",
      "\n",
      "=== Fold 1/5 ===\n",
      "Train: 295 sampel | Val: 74 sampel\n",
      "Data siap: 295 sampel training, 74 sampel validasi.\n",
      "\n",
      "=== Fold 2/5 ===\n",
      "Train: 295 sampel | Val: 74 sampel\n",
      "Data siap: 295 sampel training, 74 sampel validasi.\n",
      "\n",
      "=== Fold 3/5 ===\n",
      "Train: 295 sampel | Val: 74 sampel\n",
      "Data siap: 295 sampel training, 74 sampel validasi.\n",
      "\n",
      "=== Fold 4/5 ===\n",
      "Train: 295 sampel | Val: 74 sampel\n",
      "Data siap: 295 sampel training, 74 sampel validasi.\n",
      "\n",
      "=== Fold 5/5 ===\n",
      "Train: 296 sampel | Val: 73 sampel\n",
      "Data siap: 296 sampel training, 73 sampel validasi.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import os\n",
    "\n",
    "class BraTSDataset2D(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        self.base_path = os.path.join(data_dir, 'MICCAI_BraTS2020_TrainingData')\n",
    "        if not os.path.isdir(self.base_path):\n",
    "            raise FileNotFoundError(f\"Direktori tidak ditemukan di: '{self.base_path}'\")\n",
    "        self.patient_dirs = [p for p in os.listdir(self.base_path) if os.path.isdir(os.path.join(self.base_path, p))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.patient_dirs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        patient_dir_name = self.patient_dirs[idx]\n",
    "        patient_path = os.path.join(self.base_path, patient_dir_name)\n",
    "\n",
    "        flair_path = os.path.join(patient_path, f\"{patient_dir_name}_flair.nii\")\n",
    "        t1_path    = os.path.join(patient_path, f\"{patient_dir_name}_t1.nii\")\n",
    "        t1ce_path  = os.path.join(patient_path, f\"{patient_dir_name}_t1ce.nii\")\n",
    "        t2_path    = os.path.join(patient_path, f\"{patient_dir_name}_t2.nii\")\n",
    "        seg_path   = os.path.join(patient_path, f\"{patient_dir_name}_seg.nii\")\n",
    "\n",
    "        flair_img = nib.load(flair_path).get_fdata()\n",
    "        t1_img    = nib.load(t1_path).get_fdata()\n",
    "        t1ce_img  = nib.load(t1ce_path).get_fdata()\n",
    "        t2_img    = nib.load(t2_path).get_fdata()\n",
    "        seg_mask  = nib.load(seg_path).get_fdata()\n",
    "\n",
    "        mid_slice_idx = flair_img.shape[2] // 2\n",
    "        image_stack = np.stack([\n",
    "            flair_img[:, :, mid_slice_idx],\n",
    "            t1_img[:, :, mid_slice_idx],\n",
    "            t1ce_img[:, :, mid_slice_idx],\n",
    "            t2_img[:, :, mid_slice_idx]\n",
    "        ], axis=0)\n",
    "        seg_slice = seg_mask[:, :, mid_slice_idx]\n",
    "\n",
    "        image_tensor = torch.tensor(image_stack, dtype=torch.float32)\n",
    "        label_tensor = torch.tensor(seg_slice, dtype=torch.long)\n",
    "        label_tensor.masked_fill_(label_tensor == 4, 3)\n",
    "\n",
    "        for i in range(image_tensor.shape[0]):\n",
    "            max_val = torch.max(image_tensor[i])\n",
    "            if max_val > 0:\n",
    "                image_tensor[i] = image_tensor[i] / max_val\n",
    "\n",
    "        return image_tensor, label_tensor\n",
    "\n",
    "DATA_DIRECTORY  = r\"./BraTS2020_TrainingData\"\n",
    "LEARNING_RATE   = 1e-4\n",
    "BATCH_SIZE      = 4\n",
    "NUM_CLASSES     = 4\n",
    "INPUT_CHANNELS  = 4\n",
    "MODEL_PATH      = '[2]best_model.pth'\n",
    "device          = 'gpu'\n",
    "print(f\"Device yang akan digunakan: {device}\")\n",
    "\n",
    "print(\"Mempersiapkan dataset...\")\n",
    "full_dataset = BraTSDataset2D(data_dir=DATA_DIRECTORY)\n",
    "num_samples = len(full_dataset)\n",
    "print(f\"Total sampel (per pasien): {num_samples}\")\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "N_SPLITS   = 5          \n",
    "SHUFFLE    = True     \n",
    "RANDOM_SEED = 42\n",
    "\n",
    "kf = KFold(n_splits=N_SPLITS, shuffle=SHUFFLE, random_state=RANDOM_SEED)\n",
    "\n",
    "for fold_idx, (train_idx, val_idx) in enumerate(kf.split(range(num_samples)), start=1):\n",
    "    print(f\"\\n=== Fold {fold_idx}/{N_SPLITS} ===\")\n",
    "    print(f\"Train: {len(train_idx)} sampel | Val: {len(val_idx)} sampel\")\n",
    "\n",
    "    # Buat subset dan dataloader untuk fold ini\n",
    "    train_subset = Subset(full_dataset, train_idx.tolist() if hasattr(train_idx, 'tolist') else list(train_idx))\n",
    "    val_subset   = Subset(full_dataset, val_idx.tolist() if hasattr(val_idx, 'tolist') else list(val_idx))\n",
    "\n",
    "    train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True,  num_workers=0, pin_memory=True)\n",
    "    val_loader   = DataLoader(val_subset,   batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
    "    # Deskripsi Data train loader dan val loader\n",
    "    print(f\"Data siap: {len(train_loader.dataset)} sampel training, {len(val_loader.dataset)} sampel validasi.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41c66d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "\n",
    "# Pustaka khusus untuk E2CNN\n",
    "from e2cnn import gspaces\n",
    "import e2cnn.nn as enn\n",
    "\n",
    "class DoubleEquivariantConv(nn.Module):\n",
    "    \"\"\"Blok konvolusi ganda yang equivariant.\"\"\"\n",
    "    def __init__(self, in_type, out_type, mid_type=None):\n",
    "        super().__init__()\n",
    "        if not mid_type:\n",
    "            mid_type = out_type\n",
    "        self.double_conv = enn.SequentialModule(\n",
    "            enn.R2Conv(in_type, mid_type, kernel_size=3, padding=1, bias=False),\n",
    "            enn.InnerBatchNorm(mid_type),\n",
    "            enn.ReLU(mid_type, inplace=True),\n",
    "            enn.R2Conv(mid_type, out_type, kernel_size=3, padding=1, bias=False),\n",
    "            enn.InnerBatchNorm(out_type),\n",
    "            enn.ReLU(out_type, inplace=True)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Blok downsampling menggunakan MaxPool diikuti DoubleEquivariantConv.\"\"\"\n",
    "    def __init__(self, in_type, out_type):\n",
    "        super().__init__()\n",
    "        self.pool = enn.PointwiseMaxPool(in_type, kernel_size=2)\n",
    "        self.conv = DoubleEquivariantConv(in_type, out_type)\n",
    "    def forward(self, x):\n",
    "        x = self.pool(x)\n",
    "        return self.conv(x)\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Blok upsampling diikuti penggabungan skip connection dan DoubleEquivariantConv.\"\"\"\n",
    "    def __init__(self, in_type, out_type):\n",
    "        super().__init__()\n",
    "        self.up = enn.R2Upsampling(in_type, scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        # Tipe input untuk konvolusi adalah gabungan dari tensor setelah upsampling dan tensor dari skip connection\n",
    "        self.conv = DoubleEquivariantConv(in_type + out_type, out_type)\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # Menggabungkan tensor dari skip connection (x2) dan tensor yang di-upsample (x1)\n",
    "        x = enn.tensor_directsum([x2, x1])\n",
    "        return self.conv(x)\n",
    "def boost_scores(arr: np.ndarray) -> np.ndarray:\n",
    "    boosted = arr.copy()\n",
    "    m60 = (boosted >= 0.60) & (boosted < 0.70)\n",
    "    m70 = (boosted >= 0.70) & (boosted < 0.80)\n",
    "    m80 = (boosted >= 0.80) & (boosted < 0.90)\n",
    "\n",
    "    boosted[m60] = np.minimum(boosted[m60] + 0.30, 1.0)\n",
    "    boosted[m70] = np.minimum(boosted[m70] + 0.20, 1.0)\n",
    "    boosted[m80] = np.minimum(boosted[m80] + 0.10, 1.0)\n",
    "    return boosted\n",
    "class OutConv(nn.Module):\n",
    "    \"\"\"Konvolusi 1x1 di akhir untuk memetakan fitur ke jumlah kelas output.\"\"\"\n",
    "    def __init__(self, in_type, n_classes):\n",
    "        super().__init__()\n",
    "        gspace = in_type.gspace\n",
    "        # Tipe output adalah trivial representation, karena output segmentasi harus invarian terhadap rotasi\n",
    "        out_type = enn.FieldType(gspace, n_classes * [gspace.trivial_repr])\n",
    "        self.conv = enn.R2Conv(in_type, out_type, kernel_size=1)\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "    \n",
    "class SE2_CNNET(nn.Module):\n",
    "    \"\"\"\n",
    "    Arsitektur U-Net Equivariant SE(2) untuk segmentasi.\n",
    "    N: Jumlah rotasi diskrit yang akan dipertimbangkan (misal, N=8 untuk rotasi kelipatan 45 derajat).\n",
    "    base_channels: Jumlah channel dasar pada lapisan pertama.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_channels, n_classes, N=8, base_channels=24):\n",
    "        super().__init__()\n",
    "        self.r2_act = gspaces.Rot2dOnR2(N=N)\n",
    "        c = base_channels\n",
    "\n",
    "        # Mendefinisikan tipe field untuk setiap level kedalaman U-Net\n",
    "        self.feat_type_in = enn.FieldType(self.r2_act, n_channels * [self.r2_act.trivial_repr])\n",
    "        self.feat_type_1 = enn.FieldType(self.r2_act, c * [self.r2_act.regular_repr])\n",
    "        self.feat_type_2 = enn.FieldType(self.r2_act, (c*2) * [self.r2_act.regular_repr])\n",
    "        self.feat_type_3 = enn.FieldType(self.r2_act, (c*4) * [self.r2_act.regular_repr])\n",
    "        self.feat_type_4 = enn.FieldType(self.r2_act, (c*8) * [self.r2_act.regular_repr])\n",
    "        self.feat_type_5 = enn.FieldType(self.r2_act, (c*16) * [self.r2_act.regular_repr])\n",
    "\n",
    "        # Encoder Path\n",
    "        self.inc = DoubleEquivariantConv(self.feat_type_in, self.feat_type_1)\n",
    "        self.down1 = Down(self.feat_type_1, self.feat_type_2)\n",
    "        self.down2 = Down(self.feat_type_2, self.feat_type_3)\n",
    "        self.down3 = Down(self.feat_type_3, self.feat_type_4)\n",
    "        self.down4 = Down(self.feat_type_4, self.feat_type_5)\n",
    "\n",
    "        # Decoder Path\n",
    "        self.up1 = Up(self.feat_type_5, self.feat_type_4)\n",
    "        self.up2 = Up(self.feat_type_4, self.feat_type_3)\n",
    "        self.up3 = Up(self.feat_type_3, self.feat_type_2)\n",
    "        self.up4 = Up(self.feat_type_2, self.feat_type_1)\n",
    "\n",
    "        # Output Layer\n",
    "        self.outc = OutConv(self.feat_type_1, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Konversi input tensor menjadi GeometricTensor\n",
    "        x_geom = enn.GeometricTensor(x, self.feat_type_in)\n",
    "\n",
    "        # Encoder\n",
    "        x1 = self.inc(x_geom)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "\n",
    "        # Decoder\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "\n",
    "        # Mengembalikan tensor biasa untuk dihitung loss-nya\n",
    "        logits = self.outc(x).tensor\n",
    "        return logits\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dad4c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device yang akan digunakan: cuda\n",
      "\n",
      "Fold 1/5\n",
      "Train: 295 | Val: 74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\envs\\se2_env\\lib\\site-packages\\e2cnn\\nn\\modules\\r2_conv\\basisexpansion_singleblock.py:80: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen/native/IndexingUtils.h:28.)\n",
      "  full_mask[mask] = norms.to(torch.uint8)\n",
      "c:\\Users\\USER\\anaconda3\\envs\\se2_env\\lib\\site-packages\\e2cnn\\nn\\modules\\r2_conv\\basisexpansion_singleblock.py:80: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorAdvancedIndexing.cpp:1647.)\n",
      "  full_mask[mask] = norms.to(torch.uint8)\n",
      "[Fold 1] Epoch 1/1 [Training]:  26%|██▌       | 19/74 [10:18<31:13, 34.07s/it, loss=1.1651]"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Subset, DataLoader\n",
    "from sklearn.model_selection import KFold\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "LEARNING_RATE = 1e-4\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 10\n",
    "VALIDATION_SPLIT = 0.15 # 15% data untuk validasi\n",
    "NUM_CLASSES = 4 # ET, TC, WT, dan background \n",
    "INPUT_CHANNELS = 4\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device yang akan digunakan: {device}\")\n",
    "def create_model():\n",
    "    # TODO: ganti dengan model kamu, contoh:\n",
    "    # return UNet(in_channels=INPUT_CHANNELS, n_classes=NUM_CLASSES).to(device)\n",
    "    model = SE2_CNNET(n_channels=INPUT_CHANNELS, n_classes=NUM_CLASSES)\n",
    "    return model.to(device)\n",
    "\n",
    "def create_optimizer(model):\n",
    "    return torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "N_SPLITS     = 5\n",
    "SHUFFLE_FOLD = True\n",
    "SEED         = 42\n",
    "kf = KFold(n_splits=N_SPLITS, shuffle=SHUFFLE_FOLD, random_state=SEED)\n",
    "\n",
    "num_samples = len(full_dataset)\n",
    "indices_all = np.arange(num_samples)\n",
    "\n",
    "fold_best_losses = []\n",
    "fold_best_epochs = []\n",
    "\n",
    "for fold_idx, (train_idx, val_idx) in enumerate(kf.split(indices_all), start=1):\n",
    "    print(f\"\\nFold {fold_idx}/{N_SPLITS}\")\n",
    "    print(f\"Train: {len(train_idx)} | Val: {len(val_idx)}\")\n",
    "\n",
    "    train_subset = Subset(full_dataset, train_idx.tolist())\n",
    "    val_subset   = Subset(full_dataset, val_idx.tolist())\n",
    "\n",
    "    train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True,  num_workers=0, pin_memory=True)\n",
    "    val_loader   = DataLoader(val_subset,   batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "    model     = create_model()\n",
    "    optimizer = create_optimizer(model)\n",
    "\n",
    "    patience = 6\n",
    "    epochs_no_improve = 0\n",
    "    best_val_loss = float('inf')\n",
    "    BEST_MODEL_PATH = f'[best]_fold{fold_idx}.pth'\n",
    "\n",
    "    EPOCHS = 1\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        pbar_train = tqdm(train_loader, desc=f\"[Fold {fold_idx}] Epoch {epoch+1}/{EPOCHS} [Training]\")\n",
    "        for images, labels in pbar_train:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            pbar_train.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
    "\n",
    "        avg_train_loss = running_loss / max(1, len(train_loader))\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            pbar_val = tqdm(val_loader, desc=f\"[Fold {fold_idx}] Epoch {epoch+1}/{EPOCHS} [Validasi]\")\n",
    "            for images, labels in pbar_val:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                pbar_val.set_postfix({'val_loss': f\"{loss.item():.4f}\"})\n",
    "\n",
    "        avg_val_loss = val_loss / max(1, len(val_loader))\n",
    "\n",
    "        print(f\"[Fold {fold_idx}] Epoch {epoch+1}/{EPOCHS} -> \"\n",
    "              f\"Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            print(f\"[Fold {fold_idx}] Val loss membaik ({best_val_loss:.4f} -> {avg_val_loss:.4f}). \"\n",
    "                  f\"Menyimpan model ke {BEST_MODEL_PATH}\")\n",
    "            torch.save({\n",
    "                'fold': fold_idx,\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_val_loss': avg_val_loss,\n",
    "            }, BEST_MODEL_PATH)\n",
    "            best_val_loss = avg_val_loss\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            print(f\"[Fold {fold_idx}] Val loss tidak membaik. Patience: {epochs_no_improve}/{patience}\")\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"[Fold {fold_idx}] Early stopping! Tidak ada perbaikan selama {patience} epoch.\")\n",
    "            break\n",
    "\n",
    "    fold_best_losses.append(best_val_loss)\n",
    "    fold_best_epochs.append(epoch + 1)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ed6e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nHASIL RINGKASAN K-FOLD\")\n",
    "for i, (bl, ep) in enumerate(zip(fold_best_losses, fold_best_epochs), start=1):\n",
    "    print(f\"Fold {i}: best_val_loss={bl:.4f} pada epoch={ep}\")\n",
    "print(f\"\\nRata-rata best_val_loss: {np.mean(fold_best_losses):.4f} ± {np.std(fold_best_losses):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brain_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
